{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec638edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark Session...\n"
     ]
    }
   ],
   "source": [
    "# Core Spark Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * # Import common functions like col, split, sin, cos, etc.\n",
    "from pyspark.sql.types import * # Import data types like StringType, DoubleType, etc.\n",
    "import math # Import math for pi constant\n",
    "\n",
    "print(\"Starting Spark Session...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6423c7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/29 00:56:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Build the SparkSession (using basic configuration)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeatherPreprocessing_Notebook\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99dc28c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function 'clean_weather_column' defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Helper Function ---\n",
    "# (We define it early so subsequent cells can use it)\n",
    "def clean_weather_column(\n",
    "    input_df,\n",
    "    col_name,\n",
    "    missing_code,\n",
    "    quality_flags,\n",
    "    scale_factor,\n",
    "    handle_signs=False,\n",
    "):\n",
    "    \"\"\"Cleans NOAA value,flag columns, handling potential signs.\"\"\"\n",
    "    df_with_c = input_df.where(col(col_name).isNotNull() & col(col_name).contains(\",\"))\n",
    "    df_p = (\n",
    "        df_with_c.withColumn(f\"{col_name}_parts\", split(col(col_name), \",\"))\n",
    "        .withColumn(f\"{col_name}_value\", col(f\"{col_name}_parts\").getItem(0)) # Use getItem for safety\n",
    "        .withColumn(f\"{col_name}_flag\", col(f\"{col_name}_parts\").getItem(1))   # Use getItem for safety\n",
    "    )\n",
    "    df_good = df_p.where(\n",
    "        (col(f\"{col_name}_value\") != missing_code)\n",
    "        & (col(f\"{col_name}_flag\").isin(quality_flags))\n",
    "    )\n",
    "\n",
    "    value_col_name = f\"{col_name}_value\"\n",
    "    if handle_signs:\n",
    "        # Create a temporary column to handle '+' or '-' prefixes\n",
    "        signed_value_col = f\"{col_name}_signed_value_temp\"\n",
    "        df_good = df_good.withColumn(\n",
    "            signed_value_col,\n",
    "             # Remove '+' sign, keep '-' sign, handle others\n",
    "            regexp_replace(col(value_col_name), r\"^\\+\", \"\")\n",
    "        )\n",
    "        value_col_name = signed_value_col # Use this column for casting\n",
    "\n",
    "    clean_col_name = col_name.lower() + \"_clean\"\n",
    "    df_final = df_good.withColumn(\n",
    "        clean_col_name,\n",
    "        col(value_col_name).cast(DoubleType()) / scale_factor,\n",
    "    )\n",
    "\n",
    "    # Drop intermediate columns including the temporary signed value if created\n",
    "    drop_cols = [col_name, f\"{col_name}_parts\", f\"{col_name}_value\", f\"{col_name}_flag\"]\n",
    "    if handle_signs:\n",
    "        drop_cols.append(signed_value_col)\n",
    "\n",
    "    df_final = df_final.drop(*drop_cols)\n",
    "    return df_final\n",
    "\n",
    "print(\"Helper function 'clean_weather_column' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6958be",
   "metadata": {},
   "source": [
    "## Define Schema and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d545d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Schema ---\n",
    "# Defining the schema manually is more reliable and faster than inferSchema=True\n",
    "# Based on the columns in your provided script\n",
    "schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"SOURCE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", StringType(), True), # Load as String initially\n",
    "    StructField(\"LONGITUDE\", StringType(), True),# Load as String initially\n",
    "    StructField(\"ELEVATION\", StringType(), True),# Load as String initially\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"REPORT_TYPE\", StringType(), True),\n",
    "    StructField(\"CALL_SIGN\", StringType(), True),\n",
    "    StructField(\"QUALITY_CONTROL\", StringType(), True),\n",
    "    StructField(\"WND\", StringType(), True),\n",
    "    StructField(\"CIG\", StringType(), True),\n",
    "    StructField(\"VIS\", StringType(), True),\n",
    "    StructField(\"TMP\", StringType(), True),\n",
    "    StructField(\"DEW\", StringType(), True),\n",
    "    StructField(\"SLP\", StringType(), True),\n",
    "    # Add other string fields if needed, based on full dataset documentation\n",
    "    # For now, focusing on the ones used in your script\n",
    "    StructField(\"REM\", StringType(), True), # Example REM column often present\n",
    "    StructField(\"EQD\", StringType(), True)  # Example EQD column often present\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8858c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to one of the CSV files inside the 2024 folder\n",
    "# Replace with an actual file name if needed\n",
    "sample_file_path = \"2024/01001099999.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1808cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring schema from sample file...\n",
      "Schema inferred successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Inferring schema from sample file...\")\n",
    "inferred_schema = spark.read.option(\"header\", \"true\").csv(sample_file_path).schema\n",
    "print(\"Schema inferred successfully.\")\n",
    "# Optional: Print the full inferred schema to see all columns\n",
    "# inferred_schema.printTreeString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9df873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring schema from sample file: 2024/01001099999.csv\n",
      "Schema inferred successfully.\n",
      "Reloading data using FULL inferred schema from folder: 2024/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=================================================>   (777 + 12) / 827]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 130223689 records.\n",
      "Schema of loaded data:\n",
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AA2: string (nullable = true)\n",
      " |-- AA3: string (nullable = true)\n",
      " |-- AJ1: string (nullable = true)\n",
      " |-- AY1: string (nullable = true)\n",
      " |-- AY2: string (nullable = true)\n",
      " |-- GA1: string (nullable = true)\n",
      " |-- GA2: string (nullable = true)\n",
      " |-- GA3: string (nullable = true)\n",
      " |-- GE1: string (nullable = true)\n",
      " |-- GF1: string (nullable = true)\n",
      " |-- IA1: string (nullable = true)\n",
      " |-- KA1: string (nullable = true)\n",
      " |-- KA2: string (nullable = true)\n",
      " |-- MA1: string (nullable = true)\n",
      " |-- MD1: string (nullable = true)\n",
      " |-- MW1: string (nullable = true)\n",
      " |-- OC1: string (nullable = true)\n",
      " |-- OD1: string (nullable = true)\n",
      " |-- SA1: string (nullable = true)\n",
      " |-- UA1: string (nullable = true)\n",
      " |-- REM: string (nullable = true)\n",
      " |-- EQD: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample of raw data:\n",
      "+-----------+-------------------+------+--------+---------+---------+------------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+-----------+--------+--------+----------+----------+----------+---------------------+--------------------------+--------------------------+---------------------+------------------------------+------+---------+---------+---------+---------+------------------+------------------+------------------+-------------------------------------+\n",
      "|STATION    |DATE               |SOURCE|LATITUDE|LONGITUDE|ELEVATION|NAME                    |REPORT_TYPE|CALL_SIGN|QUALITY_CONTROL|WND           |CIG        |VIS         |TMP    |DEW    |SLP    |AA1        |AA2 |AA3 |AJ1        |AY1     |AY2     |GA1       |GA2       |GA3       |GE1                  |GF1                       |IA1                       |KA1                  |KA2                           |MA1   |MD1      |MW1      |OC1      |OD1      |SA1               |UA1               |REM               |EQD                                  |\n",
      "+-----------+-------------------+------+--------+---------+---------+------------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+-----------+--------+--------+----------+----------+----------+---------------------+--------------------------+--------------------------+---------------------+------------------------------+------+---------+---------+---------+---------+------------------+------------------+------------------+-------------------------------------+\n",
      "|99999963894|2024-01-01T00:00:00|I     |34.7728 |-87.6399 |161.5    |MUSCLE SHOALS 2 N, AL US|CRN05      |99999    |V020           |211,1,H,0018,1|99999,9,9,N|999999,9,9,9|+0105,1|+9999,9|99999,9|01,0000,9,1|NULL|NULL|05,0000,9,1|0689,1,0|0000,1,0|+01538,1,0|+01502,1,0|+01493,1,0|05,+9999,9,0,0509,1,0|0131,1,0,0107,3,9,9999,9,0|+9999,9,0,+0102,1,0,00,1,0|014988,1,0,056387,1,0|0,1,0,0000,0,0,010,4,3,010,0,8|99,-06|07001,1,0|+0105,1,0|+0105,1,0|+0105,1,0|+9999,9,0,0009,1,0|+9999,9,0,0009,1,0|+9999,9,0,0009,1,0|+0099,1,0,9999,9,0,+0125,1,0,9999,9,0|\n",
      "|99999963894|2024-01-01T00:05:00|I     |34.7728 |-87.6399 |161.5    |MUSCLE SHOALS 2 N, AL US|CRN05      |99999    |V020           |214,1,H,0017,1|99999,9,9,N|999999,9,9,9|+0100,1|+9999,9|99999,9|NULL       |NULL|NULL|05,0000,9,1|NULL    |NULL    |+01538,1,0|+01501,1,0|+01493,1,0|05,+9999,9,0,0527,1,0|NULL                      |NULL                      |NULL                 |NULL                          |99,-06|NULL     |+0100,1,0|+0100,1,0|+0100,1,0|NULL              |NULL              |NULL              |NULL                                 |\n",
      "|99999963894|2024-01-01T00:10:00|I     |34.7728 |-87.6399 |161.5    |MUSCLE SHOALS 2 N, AL US|CRN05      |99999    |V020           |212,1,H,0017,1|99999,9,9,N|999999,9,9,9|+0088,1|+9999,9|99999,9|NULL       |NULL|NULL|05,0000,9,1|NULL    |NULL    |+01538,1,0|+01500,1,0|+01493,1,0|05,+9999,9,0,0545,1,0|NULL                      |NULL                      |NULL                 |NULL                          |99,-06|NULL     |+0088,1,0|+0088,1,0|+0089,1,0|NULL              |NULL              |NULL              |NULL                                 |\n",
      "|99999963894|2024-01-01T00:15:00|I     |34.7728 |-87.6399 |161.5    |MUSCLE SHOALS 2 N, AL US|CRN05      |99999    |V020           |209,1,H,0018,1|99999,9,9,N|999999,9,9,9|+0077,1|+9999,9|99999,9|NULL       |NULL|NULL|05,0000,9,1|NULL    |NULL    |+01538,1,0|+01499,1,0|+01493,1,0|05,+9999,9,0,0570,1,0|NULL                      |NULL                      |NULL                 |NULL                          |99,-06|NULL     |+0077,1,0|+0077,1,0|+0077,1,0|NULL              |NULL              |NULL              |NULL                                 |\n",
      "|99999963894|2024-01-01T00:20:00|I     |34.7728 |-87.6399 |161.5    |MUSCLE SHOALS 2 N, AL US|CRN05      |99999    |V020           |208,1,H,0016,1|99999,9,9,N|999999,9,9,9|+0076,1|+9999,9|99999,9|NULL       |NULL|NULL|05,0000,9,1|NULL    |NULL    |+01538,1,0|+01498,1,0|+01493,1,0|05,+9999,9,0,0598,1,0|NULL                      |NULL                      |NULL                 |NULL                          |99,-06|NULL     |+0076,1,0|+0076,1,0|+0076,1,0|NULL              |NULL              |NULL              |NULL                                 |\n",
      "+-----------+-------------------+------+--------+---------+---------+------------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+-----------+--------+--------+----------+----------+----------+---------------------+--------------------------+--------------------------+---------------------+------------------------------+------+---------+---------+---------+---------+------------------+------------------+------------------+-------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 01:02:14 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 54, schema size: 39\n",
      "CSV file: file:///Users/chris/Desktop/Project/DSA5208/Project%202/2024/99999963894.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Infer the Full Schema from a Sample File ---\n",
    "\n",
    "# Get the path to one of the CSV files inside the 2024 folder\n",
    "# Make sure this file exists!\n",
    "sample_file_path = \"2024/01001099999.csv\" \n",
    "\n",
    "print(f\"Inferring schema from sample file: {sample_file_path}\")\n",
    "try:\n",
    "    # Read the header and guess the schema from just ONE file\n",
    "    inferred_schema = spark.read.option(\"header\", \"true\").csv(sample_file_path).schema\n",
    "    print(\"Schema inferred successfully.\")\n",
    "    # Optional: See all the columns Spark found\n",
    "    # inferred_schema.printTreeString() \n",
    "except Exception as e:\n",
    "    print(f\"Error inferring schema from {sample_file_path}: {e}\")\n",
    "    # Handle error, maybe try a different file or check the path\n",
    "    raise e # Stop execution if schema inference fails\n",
    "\n",
    "# --- Step 2: Load ALL Data Using the Inferred Schema ---\n",
    "data_path = \"2024/\" \n",
    "print(f\"Reloading data using FULL inferred schema from folder: {data_path}\")\n",
    "\n",
    "# Load ALL csv files using the schema Spark just figured out\n",
    "df_raw = spark.read.option(\"header\", \"true\").schema(inferred_schema).csv(data_path)\n",
    "\n",
    "# --- Initial Check ---\n",
    "print(f\"Loaded {df_raw.count()} records.\")\n",
    "print(\"Schema of loaded data:\")\n",
    "df_raw.printSchema() # Should now show all columns without errors/warnings\n",
    "print(\"\\nSample of raw data:\")\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c720dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
