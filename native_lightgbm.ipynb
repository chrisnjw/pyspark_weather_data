{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feecd5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/27 20:06:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * # Import necessary functions\n",
    "from pyspark.sql.types import * # Import necessary types\n",
    "\n",
    "# Build the basic SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GlobalHourlyWeather_NativeLGBM\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ebf6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 01001099999.csv\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000100644 0000000 0000000 00010620251 14760163177 011523\u0000 0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ustar\u000000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000000000 0000000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\"STATION\": string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AA2: string (nullable = true)\n",
      " |-- AA3: string (nullable = true)\n",
      " |-- AJ1: string (nullable = true)\n",
      " |-- AY1: string (nullable = true)\n",
      " |-- AY2: string (nullable = true)\n",
      " |-- GA1: string (nullable = true)\n",
      " |-- GA2: string (nullable = true)\n",
      " |-- GA3: string (nullable = true)\n",
      " |-- GE1: string (nullable = true)\n",
      " |-- GF1: string (nullable = true)\n",
      " |-- IA1: string (nullable = true)\n",
      " |-- KA1: string (nullable = true)\n",
      " |-- KA2: string (nullable = true)\n",
      " |-- MA1: string (nullable = true)\n",
      " |-- MD1: string (nullable = true)\n",
      " |-- MW1: string (nullable = true)\n",
      " |-- OC1: string (nullable = true)\n",
      " |-- OD1: string (nullable = true)\n",
      " |-- SA1: string (nullable = true)\n",
      " |-- UA1: string (nullable = true)\n",
      " |-- REM: string (nullable = true)\n",
      " |-- EQD: string (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------+----------+----------+---------+----------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+----+----+----+----+----+----+----+----+----+-------------+-------------+---------------+----------------+----+------+---------------+----+----+----------+----------------+\n",
      "|01001099999.csv\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000100644 0000000 0000000 00010620251 14760163177 011523\u0000 0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ustar\u000000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000000000 0000000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\"STATION\"|DATE               |SOURCE|LATITUDE  |LONGITUDE |ELEVATION|NAME                  |REPORT_TYPE|CALL_SIGN|QUALITY_CONTROL|WND           |CIG        |VIS         |TMP    |DEW    |SLP    |AA1        |AA2 |AA3 |AJ1 |AY1 |AY2 |GA1 |GA2 |GA3 |GE1 |GF1 |IA1 |KA1          |KA2          |MA1            |MD1             |MW1 |OC1   |OD1            |SA1 |UA1 |REM       |EQD             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------+----------+----------+---------+----------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+----+----+----+----+----+----+----+----+----+-------------+-------------+---------------+----------------+----+------+---------------+----+----+----------+----------------+\n",
      "|01001099999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2024-01-01T00:00:00|4     |70.9333333|-8.6666667|9.0      |JAN MAYEN NOR NAVY, NO|FM-12      |99999    |V020           |318,1,N,0061,1|99999,9,9,9|999999,9,9,9|-0070,1|-0130,1|10208,1|99,9999,9,9|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|120,M,-0069,1|120,N,-0078,1|99999,9,10196,1|0,1,000,1,+999,9|NULL|0081,1|9,99,0091,1,999|NULL|NULL|SYN004BUFR|Q01  99993PRCP99|\n",
      "|01001099999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2024-01-01T01:00:00|4     |70.9333333|-8.6666667|9.0      |JAN MAYEN NOR NAVY, NO|FM-12      |99999    |V020           |330,1,N,0051,1|99999,9,9,9|999999,9,9,9|-0065,1|-0124,1|10204,1|99,9999,9,9|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|120,M,-0064,1|120,N,-0073,1|99999,9,10192,1|8,1,007,1,+999,9|NULL|0072,1|9,99,0063,1,999|NULL|NULL|SYN004BUFR|NULL            |\n",
      "|01001099999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2024-01-01T02:00:00|4     |70.9333333|-8.6666667|9.0      |JAN MAYEN NOR NAVY, NO|FM-12      |99999    |V020           |348,1,N,0035,1|99999,9,9,9|999999,9,9,9|-0065,1|-0113,1|10205,1|99,9999,9,9|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|120,M,-0064,1|120,N,-0070,1|99999,9,10193,1|7,1,005,1,+999,9|NULL|0076,1|9,99,0060,1,999|NULL|NULL|SYN004BUFR|NULL            |\n",
      "|01001099999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2024-01-01T03:00:00|4     |70.9333333|-8.6666667|9.0      |JAN MAYEN NOR NAVY, NO|FM-12      |99999    |V020           |357,1,N,0019,1|99999,9,9,9|999999,9,9,9|-0064,1|-0105,1|10202,1|99,9999,9,9|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|120,M,-0063,1|120,N,-0066,1|99999,9,10190,1|7,1,006,1,+999,9|NULL|0064,1|9,99,0063,1,999|NULL|NULL|SYN004BUFR|NULL            |\n",
      "|01001099999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |2024-01-01T04:00:00|4     |70.9333333|-8.6666667|9.0      |JAN MAYEN NOR NAVY, NO|FM-12      |99999    |V020           |241,1,N,0008,1|99999,9,9,9|999999,9,9,9|-0070,1|-0106,1|10200,1|99,9999,9,9|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|NULL|120,M,-0065,1|120,N,-0073,1|99999,9,10188,1|8,1,004,1,+999,9|NULL|NULL  |9,99,0019,1,999|NULL|NULL|SYN004BUFR|NULL            |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+------+----------+----------+---------+----------------------+-----------+---------+---------------+--------------+-----------+------------+-------+-------+-------+-----------+----+----+----+----+----+----+----+----+----+----+----+-------------+-------------+---------------+----------------+----+------+---------------+----+----+----------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 20:10:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Update this path to where your unzipped '2024' folder is\n",
    "data_path = \"2024.tar.gz\"\n",
    "\n",
    "# Load all CSVs, using the first file's header\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Let's see what we've got!\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbee9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs. Cleaned Temperature:\n",
      "+-------+-----------+\n",
      "|    TMP|temperature|\n",
      "+-------+-----------+\n",
      "|-0070,1|       -7.0|\n",
      "|-0065,1|       -6.5|\n",
      "|-0065,1|       -6.5|\n",
      "|-0064,1|       -6.4|\n",
      "|-0070,1|       -7.0|\n",
      "|-0057,1|       -5.7|\n",
      "|-0052,1|       -5.2|\n",
      "|-0057,1|       -5.7|\n",
      "|-0047,1|       -4.7|\n",
      "|-0042,1|       -4.2|\n",
      "+-------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after cleaning TMP: 122341374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ----- THIS IS THE FIX -----\n",
    "# First, filter out any rows where TMP doesn't have a comma (or is null)\n",
    "df_with_comma = df.where(col(\"TMP\").contains(\",\"))\n",
    "# ---------------------------\n",
    "\n",
    "# 1. Split TMP into its value and quality flag\n",
    "# Now, we use df_with_comma, not df\n",
    "df_parsed = df_with_comma.withColumn(\"tmp_parts\", split(col(\"TMP\"), \",\"))\n",
    "\n",
    "# 2. Separate the value and flag into their own columns\n",
    "# This is now safe because we know a comma exists\n",
    "df_parsed = df_parsed.withColumn(\"tmp_value\", col(\"tmp_parts\")[0]) \\\n",
    "                     .withColumn(\"tmp_flag\", col(\"tmp_parts\")[1])\n",
    "\n",
    "# 3. Filter out bad data *before* scaling\n",
    "df_cleaned = df_parsed.where(\n",
    "    (col(\"tmp_value\") != \"+9999\") &\n",
    "    (col(\"tmp_flag\").isin(['1', '5']))\n",
    ")\n",
    "\n",
    "# 4. Cast to a number and apply the scaling factor (divide by 10)\n",
    "df_with_temp = df_cleaned.withColumn(\n",
    "    \"temperature\",\n",
    "    col(\"tmp_value\").cast(DoubleType()) / 10.0\n",
    ")\n",
    "\n",
    "# Let's check our work!\n",
    "print(\"Original vs. Cleaned Temperature:\")\n",
    "df_with_temp.select(\"TMP\", \"temperature\").show(10)\n",
    "\n",
    "# See how many rows we kept\n",
    "print(f\"Total records after cleaning TMP: {df_with_temp.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e86e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def clean_weather_column(input_df, col_name, missing_code, quality_flags, scale_factor):\n",
    "    \"\"\"\n",
    "    Cleans a NOAA weather column that has a 'value,flag' format.\n",
    "    \n",
    "    :param input_df: The DataFrame to transform\n",
    "    :param col_name: The name of the raw column to clean (e.g., \"DEW\", \"SLP\")\n",
    "    :param missing_code: The string code for missing values (e.g., \"+9999\", \"99999\")\n",
    "    :param quality_flags: A list of good-quality flags to keep (e.g., ['1', '5'])\n",
    "    :param scale_factor: The number to divide the value by (e.g., 10.0)\n",
    "    :return: A new DataFrame with a clean column named '<col_name>_clean'\n",
    "    \"\"\"\n",
    "    print(f\"Cleaning column: {col_name}...\")\n",
    "    \n",
    "    # 1. Filter out rows without a comma (like we did for TMP)\n",
    "    df_with_comma = input_df.where(col(col_name).contains(\",\"))\n",
    "\n",
    "    # 2. Split into value and flag\n",
    "    df_parsed = df_with_comma.withColumn(f\"{col_name}_parts\", split(col(col_name), \",\"))\n",
    "    \n",
    "    df_parsed = df_parsed.withColumn(f\"{col_name}_value\", col(f\"{col_name}_parts\")[0]) \\\n",
    "                         .withColumn(f\"{col_name}_flag\", col(f\"{col_name}_parts\")[1])\n",
    "\n",
    "    # 3. Filter out bad data\n",
    "    df_cleaned = df_parsed.where(\n",
    "        (col(f\"{col_name}_value\") != missing_code) &\n",
    "        (col(f\"{col_name}_flag\").isin(quality_flags))\n",
    "    )\n",
    "    \n",
    "    # 4. Create the final scaled, numeric column\n",
    "    clean_col_name = col_name.lower() + \"_clean\" # e.g., 'dew_clean'\n",
    "    \n",
    "    df_final = df_cleaned.withColumn(\n",
    "        clean_col_name,\n",
    "        col(f\"{col_name}_value\").cast(DoubleType()) / scale_factor\n",
    "    )\n",
    "    \n",
    "    # 5. Drop the intermediate columns\n",
    "    df_final = df_final.drop(col_name, f\"{col_name}_parts\", f\"{col_name}_value\", f\"{col_name}_flag\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ac4ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema after feature engineering (no 'month' cyclical features):\n",
      "root\n",
      " |-- 01001099999.csv\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000100644 0000000 0000000 00010620251 14760163177 011523\u0000 0\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000ustar\u000000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00000000000 0000000 \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\"STATION\": string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AA2: string (nullable = true)\n",
      " |-- AA3: string (nullable = true)\n",
      " |-- AJ1: string (nullable = true)\n",
      " |-- AY1: string (nullable = true)\n",
      " |-- AY2: string (nullable = true)\n",
      " |-- GA1: string (nullable = true)\n",
      " |-- GA2: string (nullable = true)\n",
      " |-- GA3: string (nullable = true)\n",
      " |-- GE1: string (nullable = true)\n",
      " |-- GF1: string (nullable = true)\n",
      " |-- IA1: string (nullable = true)\n",
      " |-- KA1: string (nullable = true)\n",
      " |-- KA2: string (nullable = true)\n",
      " |-- MA1: string (nullable = true)\n",
      " |-- MD1: string (nullable = true)\n",
      " |-- MW1: string (nullable = true)\n",
      " |-- OC1: string (nullable = true)\n",
      " |-- OD1: string (nullable = true)\n",
      " |-- SA1: string (nullable = true)\n",
      " |-- UA1: string (nullable = true)\n",
      " |-- REM: string (nullable = true)\n",
      " |-- EQD: string (nullable = true)\n",
      " |-- tmp_parts: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- tmp_value: string (nullable = true)\n",
      " |-- tmp_flag: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- hour_sin: double (nullable = true)\n",
      " |-- hour_cos: double (nullable = true)\n",
      " |-- day_sin: double (nullable = true)\n",
      " |-- day_cos: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the month for feature engineering\n",
    "# Make sure these imports are at the top\n",
    "from pyspark.sql.functions import sin, cos, pi, hour, month, dayofyear, col\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# 1. Parse the DATE column\n",
    "df_featured = df_with_temp.withColumn(\"timestamp\", col(\"DATE\").cast(TimestampType()))\n",
    "\n",
    "# 2. Extract base time features\n",
    "# (NOTICE 'month' IS NO LONGER EXTRACTED)\n",
    "df_with_time = df_featured.withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "                            .withColumn(\"day_of_year\", dayofyear(col(\"timestamp\")))\n",
    "\n",
    "# 3. Create cyclical features\n",
    "df_cyclical = df_with_time.withColumn(\"hour_sin\", sin(2 * pi() * col(\"hour\") / 24)) \\\n",
    "                           .withColumn(\"hour_cos\", cos(2 * pi() * col(\"hour\") / 24)) \\\n",
    "                           .withColumn(\"day_sin\", sin(2 * pi() * col(\"day_of_year\") / 366)) \\\n",
    "                           .withColumn(\"day_cos\", cos(2 * pi() * col(\"day_of_year\") / 366))\n",
    "\n",
    "# 4. Cast geographic features to numeric\n",
    "df_featured = df_cyclical.withColumn(\"latitude\", col(\"LATITUDE\").cast(DoubleType())) \\\n",
    "                         .withColumn(\"longitude\", col(\"LONGITUDE\").cast(DoubleType())) \\\n",
    "                         .withColumn(\"elevation\", col(\"ELEVATION\").cast(DoubleType()))\n",
    "\n",
    "# # 5. Run your custom 'clean_weather_column' functions\n",
    "# # (e.g., for DEW and SLP)\n",
    "# df_featured = clean_weather_column(\n",
    "#     input_df=df_featured,\n",
    "#     col_name=\"DEW\",\n",
    "#     missing_code=\"+9999\",\n",
    "#     quality_flags=['1', '5'],\n",
    "#     scale_factor=10.0\n",
    "# )\n",
    "\n",
    "# df_featured = clean_weather_column(\n",
    "#     input_df=df_featured,\n",
    "#     col_name=\"SLP\",\n",
    "#     missing_code=\"99999\",\n",
    "#     quality_flags=['1', '5'],\n",
    "#     scale_factor=10.0\n",
    "# )\n",
    "\n",
    "# 6. Check our new features\n",
    "print(\"Schema after feature engineering (no 'month' cyclical features):\")\n",
    "df_featured.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630b7e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning column: DEW...\n",
      "Cleaning column: SLP...\n",
      "+-----------+---------+---------+\n",
      "|temperature|dew_clean|slp_clean|\n",
      "+-----------+---------+---------+\n",
      "|       -7.0|    -13.0|   1020.8|\n",
      "|       -6.5|    -12.4|   1020.4|\n",
      "|       -6.5|    -11.3|   1020.5|\n",
      "|       -6.4|    -10.5|   1020.2|\n",
      "|       -7.0|    -10.6|   1020.0|\n",
      "|       -5.7|     -9.9|   1019.6|\n",
      "|       -5.2|     -9.5|   1019.6|\n",
      "|       -5.7|     -9.9|   1019.5|\n",
      "|       -4.7|     -8.6|   1019.2|\n",
      "|       -4.2|     -7.7|   1018.7|\n",
      "+-----------+---------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# We start with df_featured from the previous step\n",
    "df_final_features = df_featured\n",
    "\n",
    "# Clean the DEW column\n",
    "# Missing code: +9999, Quality flags: '1', '5', Scale: 10.0\n",
    "df_final_features = clean_weather_column(\n",
    "    input_df=df_final_features,\n",
    "    col_name=\"DEW\",\n",
    "    missing_code=\"+9999\",\n",
    "    quality_flags=['1', '5'],\n",
    "    scale_factor=10.0\n",
    ")\n",
    "\n",
    "# Clean the SLP column\n",
    "# Missing code: 99999, Quality flags: '1', '5', Scale: 10.0\n",
    "df_final_features = clean_weather_column(\n",
    "    input_df=df_final_features,\n",
    "    col_name=\"SLP\",\n",
    "    missing_code=\"99999\",\n",
    "    quality_flags=['1', '5'],\n",
    "    scale_factor=10.0\n",
    ")\n",
    "\n",
    "# Check our new clean columns\n",
    "df_final_features.select(\"temperature\", \"dew_clean\", \"slp_clean\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1efa022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|           WND|wind_speed_clean|\n",
      "+--------------+----------------+\n",
      "|318,1,N,0061,1|             6.1|\n",
      "|330,1,N,0051,1|             5.1|\n",
      "|348,1,N,0035,1|             3.5|\n",
      "|357,1,N,0019,1|             1.9|\n",
      "|241,1,N,0008,1|             0.8|\n",
      "|076,1,N,0048,1|             4.8|\n",
      "|084,1,N,0054,1|             5.4|\n",
      "|040,1,N,0023,1|             2.3|\n",
      "|098,1,N,0057,1|             5.7|\n",
      "|086,1,N,0063,1|             6.3|\n",
      "+--------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 1. Split WND into its 5 parts\n",
    "wnd_parts = split(col(\"WND\"), \",\")\n",
    "\n",
    "# 2. Get the Speed (index 3) and SpeedQuality (index 4)\n",
    "df_wind = df_final_features.withColumn(\"wind_speed_raw\", wnd_parts[3])\n",
    "df_wind = df_wind.withColumn(\"wind_speed_flag\", wnd_parts[4])\n",
    "\n",
    "# 3. Clean and scale it\n",
    "df_wind_cleaned = df_wind.where(\n",
    "    (col(\"wind_speed_raw\") != \"9999\") &\n",
    "    (col(\"wind_speed_flag\").isin(['1', '5']))\n",
    ")\n",
    "\n",
    "df_wind_final = df_wind_cleaned.withColumn(\n",
    "    \"wind_speed_clean\",\n",
    "    col(\"wind_speed_raw\").cast(DoubleType()) / 10.0  # Assuming 10.0 scale, check docs!\n",
    ")\n",
    "\n",
    "# See the result\n",
    "df_wind_final.select(\"WND\", \"wind_speed_clean\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884e66e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking statistics for new features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|summary|       temperature|         dew_clean|         slp_clean|          latitude|          longitude|         elevation|            hour_sin|            hour_cos|             day_sin|            day_cos|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          49055736|          49055736|          49055736|          49055736|           49055736|          49055736|            49055736|            49055736|            49055736|           49055736|\n",
      "|   mean|12.831891946655992| 7.024021272452314|1014.4458740646248| 32.88926715142788|-14.876760894417405|281.07311849390624|-0.00301160989368...|-0.01194671339957...|-0.00580842094277...|5.78762836549323E-4|\n",
      "| stddev|12.696444601219277|11.624523086464317| 9.060583091053223|29.368480707623316|   89.0963651980996|409.96194834192625|  0.7079346760193349|  0.7061704617978752|  0.7020131753867803| 0.7121400489000178|\n",
      "|    min|             -81.4|             -85.6|             860.6|       -82.7666666|       -179.9833333|            -999.9|                -1.0|                -1.0| -0.9999631612477099|               -1.0|\n",
      "|    max|              55.6|              36.8|            1090.0|             83.65|             179.75|            4613.0|                 1.0|                 1.0|  0.9999631612477099|                1.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Define the list of all our final numeric features\n",
    "# (We check the sin/cos features, not the originals)\n",
    "numeric_cols = [\n",
    "    \"temperature\", \n",
    "    \"dew_clean\", \n",
    "    \"slp_clean\", \n",
    "    \"latitude\", \n",
    "    \"longitude\", \n",
    "    \"elevation\", \n",
    "    \"hour_sin\", \"hour_cos\",   # Check the new features\n",
    "    \"day_sin\", \"day_cos\"      # Check the new features\n",
    "]\n",
    "\n",
    "# 2. Get the statistics for just those columns\n",
    "print(\"Checking statistics for new features:\")\n",
    "df_final_features.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a8335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+--------+---------+---------+--------+--------+-------+-------+\n",
      "|temperature|dew_clean|slp_clean|latitude|longitude|elevation|hour_sin|hour_cos|day_sin|day_cos|\n",
      "+-----------+---------+---------+--------+---------+---------+--------+--------+-------+-------+\n",
      "|          0|        0|        0|       0|        0|        0|       0|       0|      0|      0|\n",
      "+-----------+---------+---------+--------+---------+---------+--------+--------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking for Null or Nan Value\n",
    "from pyspark.sql.functions import count, when, isnan\n",
    "\n",
    "# We'll use the same list of columns from before\n",
    "cols_to_check = numeric_cols \n",
    "\n",
    "# This command counts nulls AND NaNs for each column\n",
    "df_final_features.select([\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "    for c in cols_to_check\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a4babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original count: 49055736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after elevation filter: 49045972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Define our final feature and label columns\n",
    "feature_cols = [\n",
    "    \"latitude\", \n",
    "    \"longitude\", \n",
    "    \"elevation\", \n",
    "    \"dew_clean\", \n",
    "    \"slp_clean\", \n",
    "    \"hour_sin\", \"hour_cos\",\n",
    "    \"day_sin\", \"day_cos\"\n",
    "]\n",
    "\n",
    "label_col = \"temperature\"\n",
    "\n",
    "# 2. Select only these columns AND filter out the bad elevation data\n",
    "model_df = df_final_features \\\n",
    "    .select(*feature_cols, label_col) \\\n",
    "    .where(col(\"elevation\") != -999.9)\n",
    "\n",
    "print(f\"Original count: {df_final_features.count()}\")\n",
    "print(f\"Count after elevation filter: {model_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2bb61",
   "metadata": {},
   "source": [
    "## Train Native LightGBM on sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34ba0b",
   "metadata": {},
   "source": [
    "### 1. Sample and convert to Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6668e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Sample and Convert Data ---\n",
      "Sampling 1000000 rows from Spark DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 1000000 rows to Pandas.\n",
      "\n",
      "Sample Pandas DataFrame head:\n",
      "    latitude  longitude  elevation  dew_clean  slp_clean  hour_sin  hour_cos  \\\n",
      "0  70.933333  -8.666667        9.0      -13.0     1020.8  0.000000  1.000000   \n",
      "1  70.933333  -8.666667        9.0      -12.4     1020.4  0.258819  0.965926   \n",
      "2  70.933333  -8.666667        9.0      -11.3     1020.5  0.500000  0.866025   \n",
      "3  70.933333  -8.666667        9.0      -10.5     1020.2  0.707107  0.707107   \n",
      "4  70.933333  -8.666667        9.0      -10.6     1020.0  0.866025  0.500000   \n",
      "\n",
      "    day_sin   day_cos  temperature  \n",
      "0  0.017166  0.999853         -7.0  \n",
      "1  0.017166  0.999853         -6.5  \n",
      "2  0.017166  0.999853         -6.5  \n",
      "3  0.017166  0.999853         -6.4  \n",
      "4  0.017166  0.999853         -7.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Make sure pandas is imported\n",
    "\n",
    "print(\"--- Step 1: Sample and Convert Data ---\")\n",
    "\n",
    "# 1. Sample the Spark DataFrame\n",
    "# Using .limit() is faster for a quick test.\n",
    "sample_size = 1000000 \n",
    "print(f\"Sampling {sample_size} rows from Spark DataFrame...\")\n",
    "pandas_df = model_df.limit(sample_size).toPandas()\n",
    "print(f\"Successfully converted {len(pandas_df)} rows to Pandas.\")\n",
    "\n",
    "# Display the first few rows of the Pandas DataFrame\n",
    "print(\"\\nSample Pandas DataFrame head:\")\n",
    "print(pandas_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f4fd7",
   "metadata": {},
   "source": [
    "### 2. Define features and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adfe73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Define Features and Split Data ---\n",
      "Splitting Pandas data into train/test sets...\n",
      "Training set size: 700000, Test set size: 300000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"--- Step 2: Define Features and Split Data ---\")\n",
    "\n",
    "# 1. Define features (X) and label (y) for scikit-learn/LightGBM\n",
    "# Ensure 'feature_cols' and 'label_col' were defined in a previous cell or define them here\n",
    "feature_cols = [\n",
    "    \"latitude\", \n",
    "    \"longitude\", \n",
    "    \"elevation\", \n",
    "    \"dew_clean\", \n",
    "    \"slp_clean\", \n",
    "    \"hour_sin\", \"hour_cos\",\n",
    "    \"day_sin\", \"day_cos\"\n",
    "]\n",
    "label_col = \"temperature\"\n",
    "\n",
    "X = pandas_df[feature_cols] \n",
    "y = pandas_df[label_col]\n",
    "\n",
    "# 2. Split the Pandas data into training and testing sets (70/30 split)\n",
    "print(\"Splitting Pandas data into train/test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a92f9",
   "metadata": {},
   "source": [
    "### 3. Train  Native LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce79bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Train Native LightGBM Model ---\n",
      "Training native LightGBM model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1639\n",
      "[LightGBM] [Info] Number of data points in the train set: 700000, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 6.497027\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "print(\"--- Step 3: Train Native LightGBM Model ---\")\n",
    "\n",
    "# 1. Define the LightGBM model\n",
    "lgbm_native = lgb.LGBMRegressor(objective='regression_l2', n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train the model\n",
    "print(\"Training native LightGBM model...\")\n",
    "lgbm_native.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ae7b13",
   "metadata": {},
   "source": [
    "### 4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63681238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Evaluate the Model ---\n",
      "Evaluating model on the test set...\n",
      "\n",
      "--- Native LightGBM Results (on 1000000 rows sample) ---\n",
      "Root Mean Squared Error (RMSE) = 2.0457\n",
      "R-squared (R2) = 0.9464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "print(\"--- Step 4: Evaluate the Model ---\")\n",
    "\n",
    "# 1. Make predictions on the test set\n",
    "print(\"Evaluating model on the test set...\")\n",
    "predictions = lgbm_native.predict(X_test)\n",
    "\n",
    "# 2. Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "# 3. Calculate R-squared (R2)\n",
    "r2 = lgbm_native.score(X_test, y_test)\n",
    "\n",
    "# 4. Print the results\n",
    "print(f\"\\n--- Native LightGBM Results (on {sample_size} rows sample) ---\")\n",
    "print(f\"Root Mean Squared Error (RMSE) = {rmse:.4f}\") # Format to 4 decimal places\n",
    "print(f\"R-squared (R2) = {r2:.4f}\") # Format to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dfb74",
   "metadata": {},
   "source": [
    "LightGBM was considered as an alternative model because it's a popular gradient boosting framework known for its high speed and accuracy, often outperforming standard GBTRegressor. However, integrating it directly into the PySpark pipeline presented significant challenges.\n",
    "\n",
    "Challenges with Spark Integration (SynapseML)\n",
    "1. Not Native to PySpark\n",
    "\n",
    "LightGBM requires an external library for Spark integration, unlike built-in models such as LinearRegression or GBTRegressor.\n",
    "\n",
    "2. SynapseML Dependency Conflicts\n",
    "\n",
    "We attempted using Microsoft's SynapseML library (which wraps LightGBM for Spark), but encountered critical version incompatibilities:\n",
    "\n",
    "Spark Version: Project uses Spark 4.0.1 (built on Scala 2.13).\n",
    "\n",
    "SynapseML Version: Latest stable releases (e.g., 1.0.15) target older Spark 3.x versions (built on Scala 2.12).\n",
    "\n",
    "Issue: Spark 4.x dropped support for Scala 2.12.\n",
    "\n",
    "Result:\n",
    "This mismatch caused JVM crashes (JAVA_GATEWAY_EXITED, Py4JError) when loading incompatible SynapseML JARs, preventing Spark from starting correctly.\n",
    "Even experimental SynapseML versions compatible with Scala 2.13 led to classpath conflicts that were difficult to resolve.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Workaround: Native LightGBM on Sampled Data\n",
    "\n",
    "To still evaluate LightGBMâ€™s potential, a practical workaround was adopted:\n",
    "\n",
    "1. Leverage Spark for Preprocessing:\n",
    "Use PySpark for cleaning and feature engineering on the full dataset.\n",
    "\n",
    "2. Sample Data:\n",
    "Extract a manageable sample of 1 million rows from the clean Spark DataFrame (model_df).\n",
    "\n",
    "3. Convert to Pandas:\n",
    "Transform the sample into a Pandas DataFrame for local processing.\n",
    "\n",
    "4. Use Native Libraries:\n",
    "Train LightGBM using the native lightgbm Python package with scikit-learn for splitting and evaluation.\n",
    "\n",
    "This enabled a quick and efficient test of LightGBMâ€™s performance while bypassing Spark integration issues.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Why Sampling Was Necessary (Memory Limitations) ðŸ’¾\n",
    "\n",
    "Running native LightGBM on the full dataset wasnâ€™t feasible due to memory constraints:\n",
    "\n",
    "* Pandas & RAM: Pandas must load all data into RAM.\n",
    "\n",
    "* Dataset Size: 49 million rows Ã— ~10 features far exceed typical laptop memory (8â€“32 GB).\n",
    "\n",
    "* Sparkâ€™s Advantage: Spark processes data in distributed chunks, enabling it to handle much larger datasets.\n",
    "\n",
    "Attempting .toPandas() on the full model_df would result in an â€œOut of Memoryâ€ error.\n",
    "Hence, sampling was essential for this experiment.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Alternative Solutions (for Full Dataset Integration)\n",
    "\n",
    "Integrating LightGBM with Spark at scale would typically require:\n",
    "\n",
    "1. Compatible SynapseML Version:\n",
    "Wait for or locate a SynapseML release for Spark 4.x / Scala 2.13.\n",
    "(Experimental versions may exist but can be unstable or poorly documented.)\n",
    "\n",
    "2. Managed Spark Platforms:\n",
    "Use cloud-based Spark environments like:\n",
    "\n",
    "* Databricks\n",
    "\n",
    "* Azure Synapse Analytics\n",
    "\n",
    "* AWS EMR\n",
    "\n",
    "* Google Cloud Dataproc\n",
    "These platforms simplify dependency management and may include pre-configured LightGBM support.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Recommendation for This Project\n",
    "\n",
    "Given the local environment constraints and Spark 4.x compatibility issues, integrating LightGBM directly into Spark is impractical for this project.\n",
    "\n",
    "**Recommended Approach**:\n",
    "\n",
    "1. Proceed with PySparkâ€™s native GBTRegressor:\n",
    "\n",
    "* Continue with this model for final hyperparameter tuning using CrossValidator.\n",
    "\n",
    "* Itâ€™s fully supported in PySpark and scales to the entire dataset.\n",
    "\n",
    "2. Document the Native LightGBM Experiment:\n",
    "\n",
    "* Include the Pandas/scikit-learn LightGBM results under â€œAdditional Exploration.â€\n",
    "\n",
    "* Emphasize its strong sample performance but note:\n",
    "\n",
    "    * Limited to 1M rows due to memory constraints\n",
    "\n",
    "    * Incompatibility with Spark 4.x environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
